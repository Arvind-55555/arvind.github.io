<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Attention Is All You Need — Interactive Transformer</title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet" />
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

  <style>
    code { background-color: #f4f4f4; padding: 2px 4px; border-radius: 4px; }
    pre { background: #f8f8f8; padding: 10px; border-radius: 8px; overflow-x: auto; }
    .formula { font-family: 'Cambria Math', serif; font-size: 1.05em; }
    .plot-container { margin: 2rem auto; width: 100%; max-width: 700px; }
  </style>
</head>

<body class="bg-gray-50 text-gray-900 leading-relaxed">
  <div class="max-w-5xl mx-auto py-10 px-6">
    <h1 class="text-4xl font-bold mb-6">Attention Is All You Need — Interactive Transformer</h1>

    <p class="mb-6">
      A modular <strong>Transformer implementation</strong> inspired by
      <a href="https://arxiv.org/abs/1706.03762" class="text-blue-600 underline">Vaswani et al. (2017)</a>.
      Explore the architecture, math, and interactive attention visualization below.
    </p>

    <img src="docs/assets/Attention.png" alt="Transformer Architecture" class="rounded-lg shadow-lg mb-8"/>

    <h2 class="text-2xl font-semibold mt-10 mb-3">1. Scaled Dot-Product Attention</h2>
    <p class="formula text-center mb-3">
      Attention(Q, K, V) = softmax( (QKᵀ) / √dₖ ) V
    </p>

    <pre><code class="language-python">
scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
attn = torch.softmax(scores, dim=-1)
output = torch.matmul(attn, V)
    </code></pre>

    <h2 class="text-2xl font-semibold mt-10 mb-3">2. Multi-Head Attention</h2>
    <p>
      The Transformer uses <strong>multiple attention heads</strong> to capture different relationships across tokens:
    </p>

    <p class="formula text-center my-3">
      MultiHead(Q, K, V) = Concat(head₁, ..., headₕ) Wᴼ
    </p>

    <div class="flex justify-center mb-4 space-x-4">
      <div>
        <label for="layerSelect" class="block font-semibold text-gray-700 text-sm mb-1">Layer</label>
        <select id="layerSelect" class="border rounded px-2 py-1">
          <option value="0">Layer 1</option>
          <option value="1">Layer 2</option>
        </select>
      </div>
      <div>
        <label for="headSelect" class="block font-semibold text-gray-700 text-sm mb-1">Head</label>
        <select id="headSelect" class="border rounded px-2 py-1">
          <option value="0">Head 1</option>
          <option value="1">Head 2</option>
          <option value="2">Head 3</option>
          <option value="3">Head 4</option>
        </select>
      </div>
    </div>

    <div id="attentionPlot" class="plot-container"></div>

    <h2 class="text-2xl font-semibold mt-10 mb-3">3. Train and Export</h2>
    <pre><code class="language-bash">
# Train a copy task
python -m src.train --epochs 3

# Export attention weights
python -m src.export_attention_json --checkpoint checkpoints/ckpt_epoch3.pt --out attention.json
    </code></pre>

    <hr class="my-10" />
    <p class="text-center text-gray-500">
      © 2025 Arvind | <a href="https://github.com/Arvind-55555/transformer-attention" class="text-blue-600 underline">View on GitHub</a> |
      <a href="https://arvind-55555.github.io/arvind.github.io/" class="text-blue-600 underline">← Back to Portfolio</a>
    </p>
  </div>

  <script>
    // Tokens example
    const tokens = ['[CLS]', 'The', 'cat', 'sat', 'on', 'the', 'mat', '.'];

    // Simulated attention data for visualization
    const attentionData = Array.from({ length: 2 }, () =>
      Array.from({ length: 4 }, () =>
        Array.from({ length: tokens.length }, () =>
          Array.from({ length: tokens.length }, () => Math.random())
        )
      )
    );

    function drawAttention(layer, head) {
      const z = attentionData[layer][head];
      const data = [{
        z: z,
        x: tokens,
        y: tokens,
        type: 'heatmap',
        colorscale: 'Viridis',
      }];
      const layout = {
        title: `Layer ${layer + 1} — Head ${head + 1}`,
        xaxis: { title: 'Key Tokens', side: 'top' },
        yaxis: { title: 'Query Tokens', autorange: 'reversed' },
        margin: { l: 70, r: 20, t: 80, b: 70 },
      };
      Plotly.newPlot('attentionPlot', data, layout, { responsive: true });
    }

    document.getElementById('layerSelect').addEventListener('change', (e) => {
      drawAttention(+e.target.value, +document.getElementById('headSelect').value);
    });
    document.getElementById('headSelect').addEventListener('change', (e) => {
      drawAttention(+document.getElementById('layerSelect').value, +e.target.value);
    });

    drawAttention(0, 0);
  </script>
</body>
</html>
